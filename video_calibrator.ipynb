{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3-final"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "colab": {
      "name": "video_calibrator.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOxGe_zczKMm"
      },
      "source": [
        "import os\n",
        "if not os.path.exists(\"calib_challenge\"):\n",
        "    ! git clone https://github.com/romater0/calib_challenge.git\n",
        "    ! mv calib_challenge/labeled /content "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WISe6-9szHgk"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import cv2 \n",
        "import numpy as np"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7dccPsozHgy"
      },
      "source": [
        "# Extracts the frames of a video\n",
        "temp_video_file = os.path.join('labeled', \"0.hevc\")\n",
        "REBUILD_DATA = os.path.exists('data')\n",
        "\n",
        "def extract_frames(video_file):\n",
        "    if not os.path.exists(\"data\"):\n",
        "        os.mkdir(\"data\")\n",
        "    cap = cv2.VideoCapture(video_file)   \n",
        "    frames = []\n",
        "\n",
        "    while(cap.isOpened  ()):\n",
        "        frameId = cap.get(1) \n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        if (ret != True):\n",
        "            break\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "        gray = cv2.resize(gray, (50, 50))\n",
        "        frames.append(gray)\n",
        "\n",
        "    np.save(os.path.join('data', os.path.split(video_file)[1].split('.')[0] + \".npy\"), frames)\n",
        "    \n",
        "    cap.release()\n",
        "\n",
        "if REBUILD_DATA:\n",
        "    extract_frames(temp_video_file)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "V0dMgpdHzHgz"
      },
      "source": [
        "frame_data = np.load(os.path.join(\"data\", \"0.npy\"))\n",
        "labels = np.loadtxt(os.path.join(\"labeled\", \"0.txt\"), dtype='float16')\n",
        "input_shape = frame_data.shape"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "LV4UFEPdzHg0"
      },
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ogn6qJB1Jc3"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, input_shape):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=(3,3))\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=(3,3))\n",
        "\n",
        "        x = torch.randn(input_shape).view(-1,1,input_shape[1],input_shape[2])\n",
        "        self._to_linear = None\n",
        "        self.convs(x)\n",
        "        \n",
        "        self.fcl1 = nn.Linear(self._to_linear, 512)\n",
        "        self.fcl2 = nn.Linear(512, 2)\n",
        "\n",
        "        \n",
        "\n",
        "    def convs(self, x):\n",
        "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))\n",
        "\n",
        "        if self._to_linear is None:\n",
        "            self._to_linear = x[0].shape[0]*x[0].shape[1]*x[0].shape[2]\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.convs(x)\n",
        "        x = x.view(-1, self._to_linear)\n",
        "        x = F.relu(self.fcl1(x))\n",
        "        x = self.fcl2(x) \n",
        "        return F.softmax(x, dim=1)\n",
        "\n",
        "conv_net = ConvNet(input_shape = input_shape).to(device)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anCwojll1L6V"
      },
      "source": [
        "X = torch.Tensor(frame_data).view(-1,1,input_shape[1],input_shape[2])\n",
        "X = X/255.0\n",
        "y = torch.Tensor(labels)\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8n2e1_N-WAT",
        "outputId": "f13338af-27d8-44fb-f61d-ca9edb9aa8fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "EPOCHS = 1\n",
        "BATCH_SIZE = 10\n",
        "\n",
        "def train(x, net):\n",
        "    optimizer = optim.Adam(conv_net.parameters(), lr = 0.001)\n",
        "    loss_function = nn.MSELoss()\n",
        "    for epoch in range(EPOCHS):\n",
        "        for i in tqdm(range(0, len(X), BATCH_SIZE)):\n",
        "            batch_X = X[i: i + BATCH_SIZE]\n",
        "            batch_y = y[i: i+ BATCH_SIZE]\n",
        "\n",
        "            batch_X = batch_X.to(device)\n",
        "            batch_y = batch_y.to(device)\n",
        "\n",
        "            net.zero_grad()\n",
        "\n",
        "            outputs = conv_net(batch_X)\n",
        "            loss = loss_function(outputs, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f\"Epoch: {epoch}. Loss: {loss}\")\n",
        "\n",
        "train(X, conv_net)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 120/120 [00:00<00:00, 211.71it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0. Loss: 0.2179906815290451\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4Q6CgELkUEW"
      },
      "source": [
        "# Generate validata dataset\n",
        "if not os.path.exists('data/1.npy'):\n",
        "    extract_frames('labeled/1.hevc')"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9a1aM2qDHBP"
      },
      "source": [
        "# Validation\n",
        "test_X = np.load(os.path.join(\"data\", \"1.npy\"))\n",
        "test_y = np.loadtxt(os.path.join(\"labeled\", \"1.txt\"), dtype='float16')\n",
        "test_X = torch.Tensor(test_X).view(-1,1,input_shape[1],input_shape[2])\n",
        "test_X = X/255.0\n",
        "test_y = torch.Tensor(test_y)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uLqWyYSDxzQ",
        "outputId": "df20cb19-a05a-459d-f816-8cd2b8946c1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        }
      },
      "source": [
        "def test(net):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(len(test_X))):\n",
        "            real_class = test_y[i].to(device)\n",
        "            net_out = net(test_X[i].view(-1, 1, 50, 50).to(device))[0] \n",
        "            predicted_class = torch.argmax(net_out)\n",
        "\n",
        "            if predicted_class[0] == real_class[0]:\n",
        "                correct += 1\n",
        "            total += 1\n",
        "\n",
        "    print(\"Accuracy: \", round(correct/total, 3))\n",
        "\n",
        "test(conv_net)\n",
        "\n",
        "# yeah I also think well need "
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/1200 [00:00<?, ?it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-3e247873a8c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_net\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-20-3e247873a8c1>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(net)\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mpredicted_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mpredicted_class\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mreal_class\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m                 \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: invalid index of a 0-dim tensor. Use `tensor.item()` in Python or `tensor.item<T>()` in C++ to convert a 0-dim tensor to a number"
          ]
        }
      ]
    }
  ]
}